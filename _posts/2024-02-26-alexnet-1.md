---
layout: single
title: "CNN/ AlexNet Overview"

show_date: true
categories: coding
tag: [blog, python]
toc: true

typora-root-url: ../
---



**AlexNet Overview**<br>
AlexNet is a convolutional neural network (CNN) architecture that significantly impacted the field of computer vision when it was introduced. Developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, AlexNet was the winning entry of the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC). It outperformed the second-place competitor by a large margin, demonstrating the power of CNNs in image recognition tasks and pushing forward the adoption of deep learning models in computer vision.

**Key features of AlexNet:**

**Layer Composition:** The architecture of AlexNet is made up of eight distinct layers, including five convolutional layers followed by three layers that are fully connected. At its time of introduction, such a layered structure was innovative and enabled the detection and learning of intricate patterns within the dataset.

**Activation Function Innovation:** AlexNet was among the pioneering models to incorporate the Rectified Linear Unit (ReLU) for activation, a strategy that effectively mitigated issues related to the vanishing gradient and expedited the training process.

**Innovative Pooling Method:** By implementing overlapping pooling, AlexNet could diminish its overall size and enhance generalization, thereby reducing its sensitivity to feature location and aiding in the prevention of overfitting.

**Enhancing Data Variety:** AlexNet leveraged data augmentation methods, including random cropping and horizontal flipping, to broaden the variability within its training data, thereby bolstering the model's ability to generalize.

**Preventing Overfitting with Dropout:** AlexNet utilized dropout techniques within its fully connected layers to address overfitting. This approach involves the random exclusion of network units during the training phase.

**Leveraging GPU Power:** The training of AlexNet was carried out on two NVIDIA GTX 580 GPUs over a period of one week. This dual-GPU approach was vital for managing the model's intensive computational requirements and facilitated the handling of larger models and datasets than what was previously achievable.


**Python Code for AlexNet**<br>
Below is a simplified Python code example using TensorFlow and Keras to define an AlexNet-like architecture. This code is may required adjustments for specific applications:

```python
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization

def alexnet(input_shape=(227, 227, 3), num_classes=1000):
    model = Sequential([
        Conv2D(96, (11, 11), strides=4, activation='relu', input_shape=input_shape, padding='same'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(3, 3), strides=2),
        Conv2D(256, (5, 5), activation='relu', padding='same'),
        BatchNormalization(),
        MaxPooling2D(pool_size=(3, 3), strides=2),
        Conv2D(384, (3, 3), activation='relu', padding='same'),
        Conv2D(384, (3, 3), activation='relu', padding='same'),
        Conv2D(256, (3, 3), activation='relu', padding='same'),
        MaxPooling2D(pool_size=(3, 3), strides=2),
        Flatten(),
        Dense(4096, activation='relu'),
        Dropout(0.5),
        Dense(4096, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])
    
    return model

# Example usage
model = alexnet()
model.summary()

```

This script outlines a model reminiscent of AlexNet, designed to categorize images into 1000 distinct classes. It's crucial to acknowledge that the authentic AlexNet architecture was somewhat varied, notably employing a divided GPU setup, a feature this version does not emulate. Moreover, modifications to the size of the input and the total count of output classes can be made to tailor the model to the particularities of your dataset and the objectives of your task.


**Additional Features of AlexNet**<br><br>
**Local Response Normalization (LRN):** AlexNet introduced LRN, which aimed to increase generalization by mimicking lateral inhibition in the brain. This normalization technique was applied after the ReLU activation of the first and second convolutional layers. LRN is not as commonly used in more recent architectures, as batch normalization has been found to be more effective.

**Multiple GPUs:** The original AlexNet was trained across two GPUs, which was a novel approach at the time. This setup allowed the network to handle its vast computational demands and also introduced a level of parallelism in training deep neural networks.

**Large Kernel Sizes in Initial Layers:** The first convolutional layer uses a relatively large kernel size (11x11 with a stride of 4), which was suitable for capturing features in the larger input images (e.g., 227x227 pixels) used in the network.

**Varied Kernel Sizes:** Throughout its architecture, AlexNet uses varied kernel sizes (from 11x11 to 3x3), which allow the network to capture both high-level features (using larger kernels) and more refined features (using smaller kernels) as the data progresses through the network.